{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb4_TGpDoGeV"
      },
      "source": [
        "# Project 1: Content Moderation and Toxicity Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4JI5CrRoGeX"
      },
      "source": [
        "## Pretrained Encoder-Transformers"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F8qxqGaRBFF8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#data visualisation libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pylab import rcParams\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score\n",
        "from pathlib import Path\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbVnuBryoGeZ"
      },
      "source": [
        "## Data Preprocessing\n",
        "### Data Source and Loading\n",
        "In this phase of the project, we focus on preparing our data for analysis and model training. Data preprocessing is a critical step in any machine learning pipeline, as it ensures that our data is in the right format and is accessible for processing by our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdlFB8xqy31C",
        "outputId": "4e96c4d3-5484-4b84-fab1-7f90ed4c7a61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directory already exists!\n"
          ]
        }
      ],
      "source": [
        "# Importing the warnings module to handle warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount the drive and load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Defining the path to the dataset stored in Google Drive.\n",
        "data_path = Path('/content/drive/MyDrive/Colab Notebooks/Sentiment Data/data.zip')\n",
        "\n",
        "# Defining the folder path where the extracted data will be stored.\n",
        "folder_path = Path('my_data/')\n",
        "\n",
        "# Checking if the folder already exists\n",
        "if folder_path.is_dir():\n",
        "  print(\"Directory already exists!\")\n",
        "else:\n",
        "  # If the directory does not exist, create it.\n",
        "  print(\"Making Directory\")\n",
        "  folder_path.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "# Unzipping the data\n",
        "  with zipfile.ZipFile(data_path, \"r\") as my_zip:\n",
        "    print(\"Unzipping...\")\n",
        "\n",
        "    # Extracting the data\n",
        "    my_zip.extractall(folder_path)\n",
        "    print(\"All Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP-YaaHpoGeb"
      },
      "source": [
        "## Visualization of Data Distribution\n",
        "To better understand the dataset, we explore the distribution of labels within our comments. This helps us identify imbalances and provides insights that can guide our preprocessing and model training strategies.\n",
        "\n",
        "The following visualization shows the number of occurrences for each type of label in our training data:\n",
        "\n",
        "### Code Description:\n",
        "- We load our dataset from a CSV file into a pandas DataFrame.\n",
        "- We then focus on the distribution of different labels which might represent various categories of toxicity such as 'toxic', 'severe_toxic', 'obscene', etc.\n",
        "- By calculating the sum of each label and sorting them, we prepare our data for visualization.\n",
        "- Using `matplotlib` and `seaborn`, we create a horizontal bar plot that clearly shows the frequency of each label.\n",
        "- The plot is styled with a 'viridis' color palette for visual appeal and clarity.\n",
        "- Finally, we display the plot with appropriate labels and a title.\n",
        "\n",
        "The head of the dataset, showing a few sample entries, is printed for a quick glimpse into the data structure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 713
        },
        "id": "M5rfLHnosaGh",
        "outputId": "5d59e586-5da8-4401-fa4b-60439a8494f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 id                                       comment_text  toxic  \\\n",
            "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
            "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
            "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
            "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
            "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
            "\n",
            "   severe_toxic  obscene  threat  insult  identity_hate  \n",
            "0             0        0       0       0              0  \n",
            "1             0        0       0       0              0  \n",
            "2             0        0       0       0              0  \n",
            "3             0        0       0       0              0  \n",
            "4             0        0       0       0              0  \n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data = pd.read_csv(\"/content/my_data/train.csv\")\n",
        "print(data.head())\n",
        "\n",
        "# Visualizing the class distribution of the 'label' column\n",
        "column_labels = data.columns.tolist()[2:]\n",
        "label_counts = data[column_labels].sum().sort_values()\n",
        "\n",
        "\n",
        "# Create a black background for the plot\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "# Create a horizontal bar plot using Seaborn\n",
        "ax = sns.barplot(x=label_counts.values,\n",
        "                 y=label_counts.index, palette='viridis')\n",
        "\n",
        "\n",
        "# Add labels and title to the plot\n",
        "plt.xlabel('Number of Occurrences')\n",
        "plt.ylabel('Labels')\n",
        "plt.title('Distribution of Label Occurrences')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfXZ40PxoGeb"
      },
      "source": [
        "# Data Cleaning and Sampling\n",
        "\n",
        "This step refines the dataset for training, ensuring the model learns from balanced and relevant data to improve its accuracy and reliability.\n",
        "\n",
        "## Steps Undertaken:\n",
        "\n",
        "1. **Label Summarization**: We compute the sum of instances for each label within our dataset. This gives us an overview of the distribution of different classifications such as 'toxic', 'severe_toxic', etc.\n",
        "\n",
        "2. **Separation of Data**: We categorize our comments into two groups:\n",
        "   - **Toxic Comments**: Any comment tagged with at least one 'toxic' label.\n",
        "   - **Clean Comments**: Comments without any 'toxic' labels.\n",
        "\n",
        "3. **Sampling Clean Comments**: To balance our dataset, we randomly sample a subset of the clean comments. This step is crucial to prevent model bias towards the more frequent class.\n",
        "\n",
        "4. **Merging Datasets**: We combine the toxic comments with the sampled clean comments to create a unified dataset ready for model training.\n",
        "\n",
        "5. **Shuffling**: The combined dataset is then shuffled to ensure that the order of data does not introduce any bias during the training process.\n",
        "\n",
        "6. **Splitting Data**: Finally, we extract the comment texts and labels from our dataset. We also create a validation set by splitting the data, which is used to evaluate the model's performance during training.\n",
        "\n",
        "Through these steps, we ensure that our dataset is clean, balanced, and randomized, setting the stage for effective model training. This meticulous process aids in developing robust models that generalize well to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RNE8Un5ry31G"
      },
      "outputs": [],
      "source": [
        "# Summing up the values of each column specified in column_labels and then sorting them.\n",
        "data[column_labels].sum().sort_values()\n",
        "\n",
        "# the comment is tagged with at least one toxic label.\n",
        "train_toxic = data[data[column_labels].sum(axis=1) > 0]\n",
        "# has no toxic labels and is clean.\n",
        "train_clean = data[data[column_labels].sum(axis=1) == 0]\n",
        "\n",
        "# Randomly sample 15,000 clean comments\n",
        "train_clean_sampled = train_clean.sample(n=16225, random_state=42)\n",
        "\n",
        "# Combine the toxic and sampled clean comments\n",
        "dataframe = pd.concat([train_toxic, train_clean_sampled], axis=0)\n",
        "\n",
        "# Shuffle the data to avoid any order bias during training\n",
        "dataframe = dataframe.sample(frac=1, random_state=42)\n",
        "\n",
        "# Extracting the 'comment_text' column as training texts\n",
        "train_texts = dataframe['comment_text']\n",
        "train_labels = dataframe.iloc[:,2:]\n",
        "\n",
        "# validation set\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0m7e0QNoGec"
      },
      "source": [
        "# Tokenization and Encoding\n",
        "## BERT Tokenizer\n",
        "When preparing text data for Natural Language Processing (NLP) models, especially ones based on the Transformer architecture like BERT, the process begins with tokenization and encoding.\n",
        "\n",
        "Tokenization splits the raw text into tokens (words or subwords), and encoding converts these tokens into numerical IDs that the model can understand. Special tokens like `[CLS]` for the start of the sequence and `[SEP]` for the end are added. Texts are also padded to a uniform length (`max_length`) or truncated if they're too long.\n",
        "\n",
        "An attention mask is generated to tell the model which tokens should be paid attention to and which are padding. This preprocessed data is then fed into the BERT model for training or inference.\n",
        "\n",
        "The following infographic illustrates this tokenization and encoding journey:\n",
        "\n",
        "### Implementation in Code:\n",
        "In our code:\n",
        "- We define a `tokenize_and_encode` function to handle the conversion of text to tensors that BERT can process.\n",
        "- We initialize a BERT tokenizer and model, ensuring all text is lowercased (`do_lower_case=True`) for consistency.\n",
        "- The model is transferred to the appropriate device (GPU for faster processing, if available).\n",
        "- Finally, we output the device being used to confirm where the computations will occur.\n",
        "\n",
        "Through this preparation, our text data is ready for the model to learn from or to make predictions with.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N6Temcey31H",
        "outputId": "e2c8b29d-3dc6-46fc-e3e8-3c53ce242c14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "def tokenize_and_encode(tokenizer, comments, labels, max_length=128):\n",
        "    # Initialize empty lists to store tokenized inputs and attention masks\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # Iterate through each comment in the 'comments' list\n",
        "    for comment in comments:\n",
        "\n",
        "        # Tokenize and encode the comment using the BERT tokenizer\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "            comment,\n",
        "\n",
        "            # Add special tokens like [CLS] and [SEP]\n",
        "            add_special_tokens=True,\n",
        "\n",
        "            # Truncate or pad the comment to 'max_length'\n",
        "            max_length=max_length,\n",
        "\n",
        "            # Pad the comment to 'max_length' with zeros if needed\n",
        "            pad_to_max_length=True,\n",
        "\n",
        "            # Return attention mask to mask padded tokens\n",
        "            return_attention_mask=True,\n",
        "\n",
        "            # Return PyTorch tensors\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Append the tokenized input and attention mask to their respective lists\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # Concatenate the tokenized inputs and attention masks into tensors\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "    # Convert the labels to a PyTorch tensor with the data type float32\n",
        "    labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    # Return the tokenized inputs, attention masks, and labels as PyTorch tensors\n",
        "    return input_ids, attention_masks, labels\n",
        "\n",
        "\n",
        "# Token Initialization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',\n",
        "                                          do_lower_case=True)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',\n",
        "                                                      num_labels=6)\n",
        "\n",
        "device = torch.device(\n",
        "    'cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = model.to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCN9ineOoGee"
      },
      "source": [
        "# Text Preparation for BERT\n",
        "\n",
        "In Natural Language Processing (NLP), text data must be converted into a numerical format that a model can interpret. **BERT**, a transformer-based machine learning model for NLP tasks, requires a specific format for input data which includes tokenization and encoding.\n",
        "\n",
        "## Tokenization and Encoding Explained\n",
        "\n",
        "The process begins with **tokenization**, where text is split into smaller pieces, or \"tokens.\" Each token corresponds to a word or subword in the text. BERT uses a WordPiece algorithm that allows it to handle a wide range of vocabulary without storing an enormous word-index.\n",
        "\n",
        "**Encoding** involves translating each token into a unique integer ID from BERT's vocabulary. This step is crucial as it converts text into data that can be processed by the model's neural network layers.\n",
        "\n",
        "Additionally, since BERT processes data in batches, all input sequences must be of the same length. We achieve this through **padding** (adding zeros to shorter sequences) and **truncation** (cutting off tokens from longer sequences).\n",
        "\n",
        "We also generate **attention masks** to let the model differentiate between content and padding. This mask indicates which tokens are words and should be focused on versus which ones are padding and should be ignored.\n",
        "\n",
        "## Implementing in Code\n",
        "\n",
        "Here's how we implement these steps in our code:\n",
        "\n",
        "- We call `tokenize_and_encode` to tokenize the comments and convert them to BERT's expected input format.\n",
        "- We use `TensorDataset` to wrap our input IDs, attention masks, and labels into a dataset that's easy to iterate over during training.\n",
        "- `DataLoader` objects are then created to handle shuffling and batching of our dataset, ensuring that our model can efficiently learn from the data.\n",
        "\n",
        "The outcome of this preprocessing is a set of tensors that represent our text data, ready for training or evaluating our BERT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKmLTn0Xy31I",
        "outputId": "88790314-d7d6-4614-de2c-d8ca17618657"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Comments : (16225,)\n",
            "Input Ids         : torch.Size([16225, 128])\n",
            "Attention Mask    : torch.Size([16225, 128])\n",
            "Labels            : torch.Size([16225, 6])\n"
          ]
        }
      ],
      "source": [
        "# Tokenize and encode the training texts and labels.\n",
        "# This function converts the text data into a format that is suitable for input to BERT model.\n",
        "# It includes converting texts into tokens, mapping these tokens to their index in the tokenizer vocabulary,\n",
        "# and creating attention masks to indicate which tokens should be attended to by the model.\n",
        "input_ids, attention_masks, labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    train_texts,\n",
        "    train_labels.values\n",
        ")\n",
        "\n",
        "# Tokenize and Encode the comments and labels for the validation set\n",
        "val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    val_texts,\n",
        "    val_labels.values\n",
        ")\n",
        "\n",
        "# Output the shape of the training data\n",
        "print('Training Comments :',train_texts.shape)\n",
        "print('Input Ids         :',input_ids.shape)\n",
        "print('Attention Mask    :',attention_masks.shape)\n",
        "print('Labels            :',labels.shape)\n",
        "\n",
        "# Creating DataLoader for the balanced dataset\n",
        "batch_size = 32\n",
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# validation set\n",
        "# Creating a DataLoader for the validation set.\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLVoNYo6oGef"
      },
      "source": [
        "# Training Preparation\n",
        "## Dataset and DataLoader\n",
        "\n",
        "Before diving into the actual model training, we must establish a solid foundation by initializing our training environment and data handling mechanisms.\n",
        "\n",
        "### Optimizer Initialization\n",
        "We choose the AdamW optimizer for its improved weight decay handling over the standard Adam optimizer. Weight decay is a regularization technique to prevent overfitting by penalizing large weights. The learning rate is set to `2e-5`, a common choice for fine-tuning BERT models.\n",
        "\n",
        "### Training Function\n",
        "The `train_model` function is the core of our training loop. Here's what happens inside it:\n",
        "\n",
        "- **Epochs**: We iterate over the number of epochs specified.\n",
        "- **Loss Initialization**: We start with a total loss of zero for each epoch, suming the loss as we process batches.\n",
        "- **Batch Processing**: For each batch in our DataLoader, we:\n",
        "  - Move our data (`input_ids`, `attention_masks`, and `labels`) to the active device (GPU for acceleration ).\n",
        "  - Clear previous gradients, perform a forward pass to get predictions and loss, and then a backward pass to calculate gradients.\n",
        "  - Update model parameters with `optimizer.step()`.\n",
        "- **Validation**: After training on all batches, we switch the model to evaluation mode and calculate the validation loss without updating model parameters. This gives us a measure of how well our model is generalizing.\n",
        "\n",
        "### Running the Training\n",
        "With the `train_model` function defined, we call it with our model, DataLoader, optimizer, device, and the number of epochs. The training process involves both learning from the training data and periodically validating against the validation data to monitor the model's performance.\n",
        "\n",
        "By the end of these steps, our model will have learned to predict from our data, and we will have a clear picture of its training and validation loss, guiding us towards fine-tuning or further improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uobh9DNmy31I",
        "outputId": "6f54d6fc-a956-43f4-8214-4c1595a8c2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "Epoch 1, Training Loss: 0.2169283077003449,Validation loss:0.16485132914087844\n",
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "Epoch 2, Training Loss: 0.1431237730952933,Validation loss:0.15244671082817107\n",
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "508\n",
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "Epoch 3, Training Loss: 0.11806991795356583,Validation loss:0.16281816167674415\n"
          ]
        }
      ],
      "source": [
        "# Initializing the optimizer.\n",
        "# AdamW is a variant of the Adam optimizer with fixed weight decay.\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "\n",
        "# function to train the model.\n",
        "def train_model(model, train_loader, optimizer, device, num_epochs):\n",
        "    # Loop through the specified number of epochs\n",
        "    for epoch in range(num_epochs):\n",
        "        # Set the model to training mode\n",
        "        model.train()\n",
        "        # Initialize total loss for the current epoch\n",
        "        total_loss = 0\n",
        "        i=0\n",
        "\n",
        "        # Loop through the batches in the training data\n",
        "        print(len(train_loader))\n",
        "        for batch in train_loader:\n",
        "            # using GPU or CPU\n",
        "            input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "            # Clearing out the gradients of the optimizer. Gradients need to be cleared to prevent double-counting in subsequent backpropagation.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "# Forward pass: Computing the predicted outputs by passing inputs to the model.\n",
        "            outputs = model(\n",
        "                input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            # Extracting the loss from the model's output.\n",
        "            loss = outputs.loss\n",
        "            # adding loss over all batches to calculate the total loss for the epoch.\n",
        "            total_loss += loss.item()\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Updating the model parameters based on the current gradient.\n",
        "            optimizer.step()\n",
        "            if(i%100 == 0):\n",
        "              print(i)\n",
        "            i+=1\n",
        "\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0\n",
        "\n",
        "        # Disable gradient computation during validation\n",
        "        print(len(val_loader))\n",
        "        j = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                input_ids, attention_mask, labels = [\n",
        "                    t.to(device) for t in batch]\n",
        "\n",
        "                # Forward pass with the validation data.\n",
        "                outputs = model(\n",
        "                    input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "                # getting the loss from the outputs and summing them\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "                if(j%100 == 0):\n",
        "                  print(j)\n",
        "                j+=1\n",
        "        # Print the average loss for the current epoch\n",
        "        print(\n",
        "            f'Epoch {epoch+1}, Training Loss: {total_loss/len(train_loader)},Validation loss:{val_loss/len(val_loader)}')\n",
        "\n",
        "\n",
        "# Call the function to train the model\n",
        "train_model(model, train_loader, optimizer, device, num_epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNbeQO9goGeh"
      },
      "source": [
        "# Model Evaluation\n",
        "## Evaluation Metrics\n",
        "\n",
        "After training our model, it's crucial to assess its performance using a set of evaluation metrics. This step helps us understand the effectiveness of our model in making predictions.\n",
        "\n",
        "### The Evaluation Function\n",
        "We define the `evaluate_model` function to systematically evaluate our trained model on the test data. Here's what the function does:\n",
        "\n",
        "- It switches the model to evaluation mode, which is essential for accurate performance measurement.\n",
        "- We loop through the test data, computing predictions for each batch.\n",
        "- The model's output logits are passed through a sigmoid function to obtain probabilities, which are then converted to binary predictions based on a 0.5 threshold.\n",
        "- We concatenate all the predictions and true labels to calculate global metrics such as accuracy, precision, and recall.\n",
        "- These metrics give us a quantitative measure of our model's performance:\n",
        "  - **Accuracy**: The proportion of correct predictions over the total number of cases evaluated.\n",
        "  - **Precision**: The ratio of true positive predictions to the total positive predictions.\n",
        "  - **Recall**: The ability of the model to find all the relevant cases within the dataset.\n",
        "\n",
        "By running `evaluate_model`, we receive immediate feedback on our model's performance, which is printed out for further analysis and reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DCmpXAyEy31J"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_probs = []\n",
        "\n",
        "    # Disabling gradient calculations for efficiency. Gradients are not needed during evaluation.\n",
        "    with torch.no_grad():\n",
        "        # Loop through each batch in the test data loader.\n",
        "        for batch in test_loader:\n",
        "            # GPU or CPU\n",
        "            input_ids, attention_mask, labels = [t.to(device) for t in batch]\n",
        "\n",
        "            # Forward pass: Compute the predicted outputs by passing the batch through the model.\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "            # Use sigmoid for multilabel classification (probabilities)\n",
        "            predicted_probs_batch = torch.sigmoid(outputs.logits)\n",
        "\n",
        "            # from GPU to CPU\n",
        "            predicted_probs.append(predicted_probs_batch.cpu().numpy())\n",
        "\n",
        "            true_labels_batch = labels.cpu().numpy()\n",
        "            true_labels.append(true_labels_batch)\n",
        "\n",
        "    # Combine predictions and labels for evaluation\n",
        "    true_labels = np.concatenate(true_labels, axis=0)\n",
        "    predicted_probs = np.concatenate(predicted_probs, axis=0)\n",
        "\n",
        "     # Convert probabilities to binary predictions using a threshold of 0.5.\n",
        "    predicted_labels = (predicted_probs > 0.5).astype(\n",
        "        int)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "    precision = precision_score(true_labels, predicted_labels, average='micro')\n",
        "    recall = recall_score(true_labels, predicted_labels, average='micro')\n",
        "    f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall: {recall:.4f}')\n",
        "    print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "\n",
        "# Call the function to evaluate the model on the test data\n",
        "# evaluate_model(model, test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7B9Bqm3oGeh"
      },
      "source": [
        "# Saving and Loading Model\n",
        "## Saving Process\n",
        "\n",
        "Once we have trained our model to satisfaction, the next crucial step is to save our model and tokenizer. This allows us to reload the model later to make predictions without having to retrain it from scratch, which is essential for deploying our model into a production environment or for further analysis.\n",
        "\n",
        "### How We Save the Model and Tokenizer\n",
        "\n",
        "- **Model Saving**: We save the model's weights and configuration. This includes all the parameters that have been learned during training.\n",
        "- **Tokenizer Saving**: The tokenizer is also saved, including its vocabulary and any special tokens specific to BERT.\n",
        "- Both the model and tokenizer are saved in the `Saved_model` directory. This ensures that all related files are organized and easily accessible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrKu1Wwuy31K",
        "outputId": "a9128c66-6f89-4e10-ba5b-b6fdd5d11e43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('Saved_model/tokenizer_config.json',\n",
              " 'Saved_model/special_tokens_map.json',\n",
              " 'Saved_model/vocab.txt',\n",
              " 'Saved_model/added_tokens.json')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the tokenizer and model in the same directory\n",
        "output_dir = \"Saved_model\"\n",
        "# Save model's state dictionary and configuration\n",
        "model.save_pretrained(output_dir)\n",
        "# Save tokenizer's configuration and vocabulary\n",
        "tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iflu10bfoGei"
      },
      "source": [
        "# Loading Process\n",
        "\n",
        "After saving our trained model and tokenizer, we need to be able to load them back into our working environment. This allows us to continue working with our model, whether for further training, evaluation, or making new predictions.\n",
        "\n",
        "### Steps for Loading the Model and Tokenizer\n",
        "\n",
        "- **Loading the Tokenizer**: We load the BERT tokenizer from the directory where we saved it. This ensures we have the exact tokenization scheme that was used during training.\n",
        "- **Loading the Model**: Similarly, we load the BERT model for sequence classification from the same directory. This model contains the weights and configuration from our training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScuoVVJ9y31K",
        "outputId": "1580d326-c52d-4992-ec1a-4c0783df29f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load the tokenizer and model from the saved directory\n",
        "\n",
        "# using GPU or CPU\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "# Load the tokenizer and model from the saved directory\n",
        "model_name = \"/content/Saved_model\"\n",
        "\n",
        "# Loading the BERT model for sequence classification from the saved directory.\n",
        "Bert_Tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "Bert_Model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name).to(device)\n",
        "\n",
        "# Ensuring that the model is assigned to the correct device.\n",
        "Bert_Model = Bert_Model.to(device)\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7M9GJBGoGei"
      },
      "source": [
        "# Inference and Prediction\n",
        "## Prediction Function\n",
        "\n",
        "In this section, we focus on using our trained BERT model to make predictions on new, unseen data. This process, known as inference, is crucial for evaluating how our model will perform in real-world scenarios.\n",
        "\n",
        "### The Prediction Function Explained\n",
        "\n",
        "- **Function Definition**: We define a function `predict_user_input` that takes in a text input, the model, tokenizer, and device (GPU or CPU) to perform the prediction.\n",
        "- **Tokenization**: The input text is tokenized in the same way as our training data, ensuring consistency in data format.\n",
        "- **Data Preparation**: We wrap the tokenized inputs into a `TensorDataset` and create a `DataLoader` for it. This allows us to process the input in a format that our model expects.\n",
        "- **Model Evaluation**: We set the model to evaluation mode and disable gradient calculations for efficiency.\n",
        "- **Prediction**: The model predicts the probabilities of each label for the input text. These probabilities are then converted to binary labels using a threshold of 0.5.\n",
        "- **Result Presentation**: The predictions are mapped to their corresponding label names, resulting in a dictionary that shows the likelihood of each label being applicable to the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-CxAvrAy31K",
        "outputId": "5f5a2885-24eb-4d45-af48-f6b986f8d3ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'toxic': 1,\n",
              " 'severe_toxic': 0,\n",
              " 'obscene': 0,\n",
              " 'threat': 0,\n",
              " 'insult': 1,\n",
              " 'identity_hate': 0}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def predict_user_input(input_text, model=Bert_Model, tokenizer=Bert_Tokenizer, device=device):\n",
        "    # Wrap the input text in a list. The tokenizer expects a list of texts.\n",
        "    user_input = [input_text]\n",
        "\n",
        "    # Tokenize the user input. This includes converting the text to tokens,\n",
        "    # truncating it to the maximum length the model can handle, and padding shorter texts.\n",
        "    # 'return_tensors=\"pt\"' tells the tokenizer to return PyTorch tensors.\n",
        "    user_encodings = tokenizer(\n",
        "        user_input, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "    # TensorDataset wraps tensors into a dataset and is used here to create a compatible format for DataLoader.\n",
        "    user_dataset = TensorDataset(\n",
        "        user_encodings['input_ids'], user_encodings['attention_mask'])\n",
        "\n",
        "    # DataLoader is used to create an iterable over the dataset with a specified batch size.\n",
        "    # Since we are predicting for a single input, batch_size is set to 1.\n",
        "    user_loader = DataLoader(user_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Set the model to evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in user_loader:\n",
        "            # GPUT or CPU\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "\n",
        "            # Get model's predictions for the batch.\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Apply sigmoid function to the output logits to obtain probabilities.\n",
        "            predictions = torch.sigmoid(logits)\n",
        "\n",
        "    # Convert the predictions to binary labels using a threshold (0.5)\n",
        "    predicted_labels = (predictions.cpu().numpy() > 0.5).astype(int)\n",
        "\n",
        "    # Define a list of label names corresponding to the outputs.\n",
        "    labels_list = ['toxic', 'severe_toxic', 'obscene',\n",
        "                   'threat', 'insult', 'identity_hate']\n",
        "\n",
        "        # Create a dictionary mapping label names to their predicted values.\n",
        "    result = dict(zip(labels_list, predicted_labels[0]))\n",
        "    return result\n",
        "\n",
        "# examples\n",
        "text = 'Are you insane!'\n",
        "predict_user_input(input_text=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSTK38Ghy31L",
        "outputId": "5b0e6f78-3f8d-46c5-e772-1a0145aef0a5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'toxic': 1,\n",
              " 'severe_toxic': 0,\n",
              " 'obscene': 1,\n",
              " 'threat': 1,\n",
              " 'insult': 1,\n",
              " 'identity_hate': 1}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predict_user_input(input_text='I will kill you nigga')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R57iRiCyoGej"
      },
      "source": [
        "# Reading and Filtering Test Data\n",
        "\n",
        "The final step in preparing our dataset involves reading the test data and ensuring it's in the correct format for model evaluation.\n",
        "\n",
        "## Reading Test Data\n",
        "First, we read our test data and its corresponding labels from CSV files. This data is crucial for objectively assessing the performance of our trained model.\n",
        "\n",
        "### Filtering the Test Labels\n",
        "Not all rows in the test data might be relevant or properly labeled. We specifically filter out rows where the 'toxic' column has a value of `-1`, which likely indicates missing or irrelevant data. This step ensures that we only consider rows with proper labels for evaluation.\n",
        "\n",
        "### Matching Test Data and Labels\n",
        "We then align the test data (`test_texts`) with the filtered labels (`test_labels`). This alignment is crucial to maintain the correspondence between the input texts and their respective labels, a vital aspect for accurate model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YHPnK32ioMZ0"
      },
      "outputs": [],
      "source": [
        "# Reading Test data from the csv files\n",
        "test_data = pd.read_csv(\"/content/my_data/test.csv\")\n",
        "test_labels = pd.read_csv(\"/content/my_data/test_labels.csv\")\n",
        "\n",
        "# Filtering the test labels   to exclude rows where the 'toxic' column has a value of -1.\n",
        "# The value -1 in this context likely indicates missing or irrelevant data\n",
        "test_labels = test_labels[test_labels['toxic'] != -1]\n",
        "\n",
        "# Selecting the rows in 'test_data' that have corresponding labels in 'test_labels'.\n",
        "test_texts = test_data[test_data['id'].isin(test_labels['id'])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtSl9IKXoGek"
      },
      "source": [
        "# Preparing Test Data for BERT\n",
        "After filtering and aligning our test data, the next step is to prepare it for evaluation using the BERT model. This involves tokenizing and encoding the data in a format that BERT can understand and process.\n",
        "\n",
        "## Tokenization and Encoding Process\n",
        "The tokenization and encoding are handled by the `tokenize_and_encode` function, which performs several critical steps:\n",
        "\n",
        "1. **Tokenization with BertTokenizer**:\n",
        "   - The `BertTokenizer` instance converts the raw text data into tokens. Tokenization is the process of breaking down text into smaller pieces that the model can work with.\n",
        "\n",
        "2. **Processing Text Data**:\n",
        "   - We process the text data (`comment_text`) from our `test_texts` DataFrame. This text is what we need our model to evaluate.\n",
        "\n",
        "3. **Handling Labels**:\n",
        "   - The labels associated with our test data are also prepared. We use `iloc[:,1:].values` to select all rows but only columns with labels, converting them into a NumPy array for easy handling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LgnbyvuGtkSp"
      },
      "outputs": [],
      "source": [
        "# Tokenize and encode the test dataset using the BERT tokenizer.\n",
        "# This is a crucial step to prepare the textual data for processing by the BERT model.\n",
        "test_input_ids, test_attention_masks, test_labels = tokenize_and_encode(\n",
        "    tokenizer,\n",
        "    test_texts['comment_text'],\n",
        "    test_labels.iloc[:,1:].values\n",
        ")\n",
        "\n",
        "# Tokenizing and encoding the test data using BERT tokenizer.\n",
        "\n",
        "# 'tokenize_and_encode' is used with:\n",
        "# 1. tokenizer: BertTokenizer instance for text conversion.\n",
        "# 2. test_texts['comment_text']: Text data from the test dataset.\n",
        "# 3. test_labels.iloc[:,1:].values: Corresponding labels for the test data as a NumPy array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_igf2e7GoGek"
      },
      "source": [
        "# Preparing Test Data for Model Evaluation\n",
        "Once the test dataset has been tokenized and encoded, it's crucial to set it up in a way that's compatible with our BERT model for evaluation. This involves creating a `TensorDataset` and a `DataLoader`.\n",
        "\n",
        "## Creating TensorDataset\n",
        "A `TensorDataset` wraps the tokenized and encoded data (`test_input_ids`, `test_attention_masks`, and `test_labels`) into a single dataset. This format is particularly useful for PyTorch models, as it allows for easy batching and access during the evaluation process.\n",
        "\n",
        "## DataLoader Setup\n",
        "The `DataLoader` is responsible for feeding this data to our model during evaluation. Key points in its setup include:\n",
        "\n",
        "- **Batch Size**: We choose a batch size of 32, meaning the model will process 32 examples at a time. This number is a balance between memory usage and performance.\n",
        "- **Shuffling**: For the test set, we set `shuffle=False` to maintain the order of the data. This is important for evaluation purposes, as we want to compare our predictions directly with the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9Z867mbkv9IL"
      },
      "outputs": [],
      "source": [
        "# Prepare the test data for the model.\n",
        "# This combines the test text, attention masks, and labels into a single dataset.\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
        "\n",
        "# Set up for loading the test data.\n",
        "# DataLoader helps to go through the test data in small groups (batches) of 32 at a time,\n",
        "# without mixing up the order of the data.\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDC4gEjBoGel"
      },
      "source": [
        "# Model Evaluation\n",
        "\n",
        "After training and preparing our model, it's essential to evaluate its performance to understand how well it has learned to predict the desired outcomes.\n",
        "\n",
        "## Evaluating the Trained Model\n",
        "We use the `evaluate_model` function to assess the model's effectiveness. This function will put the model through its paces with the test data and provide us with key performance metrics.\n",
        "\n",
        "### Key Metrics for Evaluation\n",
        "The function evaluates the model on various metrics, such as:\n",
        "\n",
        "- **Accuracy**: Measures the proportion of correct predictions.\n",
        "- **Precision and Recall**: Evaluate the model's ability to correctly identify positive instances and its sensitivity to detecting positive labels, respectively.\n",
        "\n",
        "These metrics give us a quantifiable understanding of how well our model performs and are crucial for gauging its effectiveness in real-world applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5bYAdpIx06o",
        "outputId": "8014584a-7eee-4edc-b629-a07a53bc6823"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained Model Stats =\n",
            "Accuracy: 0.7483\n",
            "Precision: 0.3987\n",
            "Recall: 0.8829\n",
            "F1 Score: 0.5493\n"
          ]
        }
      ],
      "source": [
        "print(\"Trained Model Stats =\")\n",
        "# Use the 'evaluate_model' function to check how well the trained model is doing.\n",
        "# It will test the model with the test data and show results like accuracy and other scores.\n",
        "evaluate_model(model, test_loader, device)\n",
        "\n",
        "# print(\"Loaded Model Stats =\")\n",
        "# evaluate_model(Bert_Model, test_loader, device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
